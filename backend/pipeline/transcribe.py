from __future__ import annotations

import copy
import time
import math
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

from .config import PipelineConfig
from .instrumentation import PipelineLogger
from .models import AnalysisData, NoteEvent, TranscriptionResult, FramePitch
from .stage_a import load_and_preprocess
from .stage_b import compute_decision_trace, extract_features
from .stage_c import apply_theory
from .stage_d import quantize_and_render
from .neural_transcription import transcribe_onsets_frames


def _quality_metrics(
    notes: List[NoteEvent],
    duration_sec: float,
    *,
    timeline_source: Optional[List[Any]] = None,
) -> Dict[str, Any]:
    duration_sec = float(max(1e-6, duration_sec or 0.0))
    note_count = int(len(notes or []))
    if note_count == 0:
        return {
            "voiced_ratio": 0.0,
            "note_count": 0,
            "notes_per_sec": 0.0,
            "median_note_dur_ms": 0.0,
            "fragmentation_lt_80ms": 0.0,
        }

    durs = []
    tiny = 0
    total_note_dur = 0.0
    for n in notes:
        dur = float(getattr(n, "end_sec", 0.0)) - float(getattr(n, "start_sec", 0.0))
        dur = max(0.0, dur)
        durs.append(dur)
        total_note_dur += dur
        if dur < 0.08:
            tiny += 1

    durs_sorted = sorted(durs)
    mid = len(durs_sorted) // 2
    median_dur = durs_sorted[mid] if durs_sorted else 0.0

    # voiced_ratio:
    # - prefer detector timeline if available (classic path)
    # - else fallback to coverage ratio (E2E path)
    voiced_ratio: Optional[float] = None
    if timeline_source is not None:
        total_frames = max(1, len(timeline_source))
        voiced_frames = 0
        frames_with_active_attr = 0
        for fr in timeline_source:
            ap = getattr(fr, "active_pitches", None)
            if ap is None:
                continue
            frames_with_active_attr += 1
            if len(ap) > 0:
                voiced_frames += 1
        if frames_with_active_attr > 0:
            voiced_ratio = float(voiced_frames / total_frames)

    if voiced_ratio is None:
        voiced_ratio = float(min(1.0, total_note_dur / duration_sec))

    return {
        "voiced_ratio": float(max(0.0, min(1.0, voiced_ratio))),
        "note_count": note_count,
        "notes_per_sec": float(note_count / duration_sec),
        "median_note_dur_ms": float(1000.0 * median_dur),
        "fragmentation_lt_80ms": float(tiny / max(1, note_count)),
    }


def _quality_score(metrics: Dict[str, Any]) -> float:
    """
    Deterministic scalar score in [0, 1].
    Intended to reject: (a) empty output, (b) extreme fragmentation, (c) crazy note rate.
    """
    if int(metrics.get("note_count", 0) or 0) <= 0:
        return 0.0

    voiced = float(metrics.get("voiced_ratio", 0.0) or 0.0)
    nps = float(metrics.get("notes_per_sec", 0.0) or 0.0)
    med_ms = float(metrics.get("median_note_dur_ms", 0.0) or 0.0)
    frag = float(metrics.get("fragmentation_lt_80ms", 0.0) or 0.0)

    # normalize median duration: 0ms..200ms -> 0..1 (cap)
    dur_term = max(0.0, min(1.0, med_ms / 200.0))

    # note rate penalty (prefer <= ~12 notes/sec)
    rate_term = 1.0 - max(0.0, min(1.0, (nps - 6.0) / 12.0))  # 6->1.0, 18->0.0

    # fragmentation penalty
    frag_term = 1.0 - max(0.0, min(1.0, frag))

    score = 0.45 * voiced + 0.25 * dur_term + 0.20 * rate_term + 0.10 * frag_term
    return float(max(0.0, min(1.0, score)))


def _candidate_order(routed_mode: str) -> List[str]:
    """
    Deterministic fallback chain.
    (We don’t run everything; we run until we find an accepted candidate.)

    NOTE: These mode ids must match your routing + Stage B logic.
    """
    all_modes = [
        "e2e_onsets_frames",
        "e2e_basic_pitch",
        "classic_piano_poly",
        "classic_song",
        "classic_melody",
    ]
    routed_mode = routed_mode if routed_mode in all_modes else "classic_melody"
    rest = [m for m in all_modes if m != routed_mode]
    return [routed_mode] + rest


def _safe_trace(x: Any) -> Dict[str, Any]:
    if x is None:
        return {}
    if isinstance(x, dict):
        return x
    return {"raw": x}


def _assemble_resolved_params(
    cand_cfg: PipelineConfig,
    meta: Any,
    decision_trace: Dict[str, Any],
    candidate_id: str,
    cand_score: float,
    cand_metrics: Dict[str, Any],
    quality_gate_cfg: Dict[str, Any],
    hop_sec: float,
    tb_source: str,
    timeline_source: str,
    time_grid: Optional[List[float]],
) -> Dict[str, Any]:
    """Assemble the unified resolved parameter view."""
    
    def _extract_routing_reasons(dt: Dict[str, Any]) -> List[str]:
        return dt.get("routing_reasons", []) or [r["rule_id"] for r in dt.get("rule_hits", []) if r.get("passed")]

    resolved = {
        "timebase": {
            "sample_rate": getattr(meta, "sample_rate", 0),
            "hop_length": getattr(meta, "hop_length", 0),
            "window_size": getattr(meta, "window_size", 0),
            "frame_hop_seconds": float(hop_sec),
            "frame_hop_seconds_source": str(tb_source),
            "timeline_source": str(timeline_source),
            "time_grid_available": bool(time_grid is not None and len(time_grid) > 0),
        },
        "preprocessing": {
            "lc_filter_mode": "on" if getattr(cand_cfg.stage_a, "high_pass_filter", {}).get("enabled") else "off",
            "target_sr": getattr(cand_cfg.stage_a, "target_sample_rate", None),
        },
        "detectors": {
            "ensemble_weights": dict(getattr(cand_cfg.stage_b, "ensemble_weights", {})),
            "detectors_config": dict(getattr(cand_cfg.stage_b, "detectors", {})),
        },
        "segmentation": {
            "segmentation_method": getattr(cand_cfg.stage_c, "segmentation_method", "threshold"),
            "onsets_enabled": getattr(cand_cfg.stage_c, "use_onset_refinement", True),
            "polyphony_enabled": bool(getattr(cand_cfg.stage_c.polyphony_filter, "mode", "off") != "off"),
        },
        "post_processing": {
            "quantize_grid": getattr(cand_cfg.stage_d, "quantization_grid", 16),
            "quantize_mode": getattr(cand_cfg.stage_d, "quantization_mode", "grid"),
            "chord_snap_ms": getattr(cand_cfg.stage_c, "chord_onset_snap_ms", 25.0),
            "merge_gap_ms": getattr(cand_cfg.stage_c.gap_filling, "max_gap_ms", 60.0),
        },
        "scoring_routing": {
            "candidate_id": str(candidate_id),
            "quality_score": float(cand_score),
            "quality_metrics": dict(cand_metrics),
            "quality_gate_threshold": float(quality_gate_cfg.get("threshold", 0.45)),
            "quality_gate_enabled": bool(quality_gate_cfg.get("enabled", True)),
            "routing_reasons": _extract_routing_reasons(decision_trace),
            "decision_trace": decision_trace,
        },
    }
    return resolved


def transcribe(
    audio_path: str,
    config: Optional[PipelineConfig] = None,
    pipeline_logger: Optional[PipelineLogger] = None,
    device: str = "cpu",
    *,
    # explicit caller overrides
    requested_mode: Optional[str] = None,
    requested_profile: Optional[str] = None,
    requested_separation_mode: Optional[str] = None,
) -> TranscriptionResult:
    """
    High-level transcription entry point with unified quality gate.

    Callers can force intended mode explicitly:
        transcribe(..., requested_mode="classic_song")
    """
    if config is None:
        config = PipelineConfig()
    if pipeline_logger is None:
        pipeline_logger = PipelineLogger()

    t0 = time.time()

    # ---------------- Stage A ----------------
    stage_a_out = load_and_preprocess(audio_path, config, pipeline_logger=pipeline_logger)
    duration_sec = float(getattr(getattr(stage_a_out, "meta", None), "duration_sec", 0.0) or 0.0)

    # Resolve caller intent (robust)
    cfg_mode = "auto"
    try:
        cfg_mode = getattr(config.stage_b, "transcription_mode", "auto")
    except Exception:
        cfg_mode = "auto"

    req_mode = str(requested_mode) if requested_mode is not None else str(cfg_mode)

    # B1: Preserve Stage A texture if caller passed "quality"/"fast"
    requested_quality_mode = None
    meta_processing_mode_preserved = False
    meta_processing_mode_val = None

    if requested_mode in ("quality", "fast"):
        requested_quality_mode = requested_mode
        # Ensure we don't clobber meta.processing_mode
        meta_processing_mode_preserved = True
        try:
            meta_processing_mode_val = stage_a_out.meta.processing_mode
        except Exception:
            pass

    meta_profile = str(getattr(getattr(stage_a_out, "meta", None), "instrument", None) or "unknown")
    req_profile = str(requested_profile) if requested_profile is not None else meta_profile

    # ---------------- Routing (Stage B trace only) ----------------
    base_trace = compute_decision_trace(
        stage_a_out,
        config,
        requested_mode=req_mode,
        requested_profile=req_profile,
        requested_separation_mode=requested_separation_mode,
        pipeline_logger=pipeline_logger,
    )

    if req_mode and req_mode != "auto":
        routed_mode = req_mode
    else:
        routed_mode = str(_safe_trace(base_trace).get("resolved", {}).get("transcription_mode", "classic_melody"))

    candidate_ids = _candidate_order(routed_mode)

    # ---------------- Unified Quality Gate ----------------
    qcfg = getattr(config, "quality_gate", None)
    if not isinstance(qcfg, dict):
        qcfg = {}
    q_enabled = bool(qcfg.get("enabled", True))
    q_threshold = float(qcfg.get("threshold", 0.45))
    q_max_candidates = int(qcfg.get("max_candidates", 3))

    candidates: List[Dict[str, Any]] = []
    fallbacks_triggered: List[str] = []

    best: Optional[Tuple[str, float, AnalysisData, PipelineConfig]] = None

    # Mix audio for E2E paths
    mix_audio = None
    try:
        mix_audio = stage_a_out.stems["mix"].audio
    except Exception:
        mix_audio = None

    for idx, cand_id in enumerate(candidate_ids):
        if idx >= q_max_candidates:
            break

        cand_score = 0.0
        cand_metrics: Dict[str, Any] = {}
        cand_analysis: Optional[AnalysisData] = None
        cand_trace: Dict[str, Any] = {}

        try:
            cand_cfg = copy.deepcopy(config)

            # Force candidate mode
            try:
                cand_cfg.stage_b.transcription_mode = cand_id
            except Exception:
                pass

            if cand_id == "e2e_onsets_frames":
                enabled = bool(getattr(cand_cfg.stage_b, "onsets_and_frames", {}).get("enabled", False))
                if not enabled:
                    raise RuntimeError("onsets_frames_disabled")
                if mix_audio is None:
                    raise RuntimeError("missing_mix_audio")

                notes_res, diag_res = transcribe_onsets_frames(
                    mix_audio,
                    int(getattr(stage_a_out.meta, "sample_rate", 44100)),
                    cand_cfg
                )
                notes = notes_res
                cand_trace.update(diag_res)

                cand_trace = _safe_trace(
                    compute_decision_trace(
                        stage_a_out,
                        cand_cfg,
                        requested_mode="e2e_onsets_frames",
                        requested_profile=req_profile,
                        requested_separation_mode=requested_separation_mode,
                        pipeline_logger=pipeline_logger,
                    )
                )

                cand_analysis = AnalysisData(
                    meta=stage_a_out.meta,
                    timeline=[],
                    stem_timelines={},
                    notes=list(notes),
                    notes_before_quantization=list(notes),
                    chords=[],
                    diagnostics={"decision_trace": cand_trace},
                )

            else:
                # Classic + BasicPitch go through Stage B -> Stage C
                stage_b_out = extract_features(
                    stage_a_out, config=cand_cfg, pipeline_logger=pipeline_logger, device=device
                )

                sb_diag = getattr(stage_b_out, "diagnostics", None) or {}
                cand_trace = _safe_trace(sb_diag.get("decision_trace", {}))

                # B2: Timeline fallback logic
                sb_timeline = getattr(stage_b_out, "timeline", None) or []
                sb_stem_timelines = getattr(stage_b_out, "stem_timelines", None) or {}

                timeline_source = "empty"
                final_timeline = []
                timeline_synth_diag = {}

                if sb_stem_timelines:
                    timeline_source = "stems"
                    final_timeline = sb_timeline # Use existing if present, or let AnalysisData handle
                elif sb_timeline:
                    timeline_source = "stage_b_timeline"
                    final_timeline = sb_timeline
                else:
                    # Fallback synthesis
                    time_grid = getattr(stage_b_out, "time_grid", None)
                    f0_main = getattr(stage_b_out, "f0_main", None)
                    if time_grid is not None and f0_main is not None and len(time_grid) == len(f0_main):
                        timeline_source = "synth_from_time_grid"
                        timeline_synth_diag = {"used_time_grid": True, "used_f0_main": True, "confidence": "assumed_1.0"}

                        synths = []
                        for t_idx, t_val in enumerate(time_grid):
                            f0 = float(f0_main[t_idx])
                            voiced = f0 > 0.0

                            midi_val = None
                            if voiced:
                                try:
                                    midi_val = int(round(69.0 + 12.0 * math.log2(f0 / 440.0)))
                                except Exception:
                                    pass

                            synths.append(FramePitch(
                                time=float(t_val),
                                pitch_hz=f0,
                                midi=midi_val,
                                confidence=1.0 if voiced else 0.0,
                                rms=0.0,
                                active_pitches=[(f0, 1.0)] if voiced else []
                            ))
                        final_timeline = synths

                # B3: Frame hop seconds calculation
                frame_hop_seconds = 0.0
                frame_hop_source = "unknown"

                time_grid_ref = getattr(stage_b_out, "time_grid", None)
                if time_grid_ref is not None and len(time_grid_ref) >= 2:
                    frame_hop_seconds = float(np.median(np.diff(time_grid_ref)))
                    frame_hop_source = "stage_b_time_grid"
                else:
                    try:
                        frame_hop_seconds = float(stage_b_out.meta.hop_length) / float(stage_b_out.meta.sample_rate)
                        frame_hop_source = "meta"
                    except Exception:
                        pass

                cand_analysis = AnalysisData(
                    meta=stage_b_out.meta,
                    timeline=final_timeline,
                    stem_timelines=sb_stem_timelines,
                    diagnostics=sb_diag,
                    precalculated_notes=getattr(stage_b_out, "precalculated_notes", None),
                    n_frames=len(final_timeline),
                    frame_hop_seconds=frame_hop_seconds
                )

                # Restore processing_mode if needed (B1 fix)
                if requested_quality_mode and meta_processing_mode_val is not None:
                    try:
                        cand_analysis.meta.processing_mode = meta_processing_mode_val
                    except Exception:
                        pass

                cand_analysis.diagnostics = cand_analysis.diagnostics or {}
                cand_analysis.diagnostics["timeline_source"] = timeline_source
                cand_analysis.diagnostics["timeline_frames"] = len(final_timeline)
                if timeline_synth_diag:
                    cand_analysis.diagnostics["timeline_synth"] = timeline_synth_diag
                cand_analysis.diagnostics["frame_hop_seconds_source"] = frame_hop_source
                
                # REPORT 3: Unified resolved params (pre-Stage C)
                # We calculate metrics/score temporarily just for the snapshot (final score used for gate logic)
                _tmp_metrics = _quality_metrics([], duration_sec) # Placeholder, updated later if needed
                
                resolved_params = _assemble_resolved_params(
                    cand_cfg,
                    stage_b_out.meta,
                    cand_trace,
                    cand_id,
                    0.0, # Score placeholder
                    _tmp_metrics,
                    qcfg,
                    frame_hop_seconds,
                    frame_hop_source,
                    timeline_source,
                    time_grid_ref
                )
                cand_analysis.diagnostics["resolved_params"] = resolved_params

                # Stage C — IMPORTANT: capture returned notes
                notes_pred = apply_theory(cand_analysis, cand_cfg)

                # Ensure notes are actually attached (handles both “returns list” and “mutates in place” implementations)
                if notes_pred is not None:
                    try:
                        cand_analysis.notes = list(notes_pred)
                    except Exception:
                        pass

                if not getattr(cand_analysis, "notes_before_quantization", None):
                    try:
                        cand_analysis.notes_before_quantization = list(getattr(cand_analysis, "notes", []) or [])
                    except Exception:
                        pass

                # Ensure decision trace visible at top-level
                try:
                    cand_analysis.diagnostics = cand_analysis.diagnostics or {}
                    cand_analysis.diagnostics["decision_trace"] = cand_trace
                except Exception:
                    pass

            # Score candidate
            notes_raw = list(cand_analysis.notes_before_quantization or cand_analysis.notes or [])
            timeline_src = cand_analysis.timeline if (cand_analysis.timeline and len(cand_analysis.timeline) > 0) else None
            cand_metrics = _quality_metrics(notes_raw, duration_sec, timeline_source=timeline_src)
            cand_score = _quality_score(cand_metrics)

            # Update resolved params with actual score/metrics
            if cand_analysis and getattr(cand_analysis, "diagnostics", None):
                 rp = cand_analysis.diagnostics.get("resolved_params")
                 if rp:
                     rp["scoring_routing"]["quality_score"] = float(cand_score)
                     rp["scoring_routing"]["quality_metrics"] = dict(cand_metrics)

            accepted = (not q_enabled) or (
                int(cand_metrics.get("note_count", 0)) > 0 and cand_score >= q_threshold
            )
            cand_decision = "accepted" if accepted else "rejected"
            cand_reason = "ok" if accepted else "below_threshold"

            if best is None or cand_score > best[1]:
                best = (cand_id, cand_score, cand_analysis, cand_cfg)

            candidates.append(
                {
                    "candidate_id": cand_id,
                    "score": float(cand_score),
                    "metrics": dict(cand_metrics),
                    "decision": cand_decision,
                    "reason": cand_reason,
                }
            )

            if accepted:
                break

            fallbacks_triggered.append(f"fallback_from_{cand_id}")

        except Exception as e:
            fallbacks_triggered.append(f"error_{cand_id}")
            candidates.append(
                {
                    "candidate_id": cand_id,
                    "score": 0.0,
                    "metrics": {
                        "voiced_ratio": 0.0,
                        "note_count": 0,
                        "notes_per_sec": 0.0,
                        "median_note_dur_ms": 0.0,
                        "fragmentation_lt_80ms": 0.0,
                    },
                    "decision": "rejected",
                    "reason": f"error:{type(e).__name__}",
                }
            )

    if best is None:
        analysis_data = AnalysisData(
            meta=stage_a_out.meta,
            timeline=[],
            stem_timelines={},
            notes=[],
            notes_before_quantization=[],
            chords=[],
            diagnostics={},
        )
        selected_id = "none"
        selected_score = 0.0
        selected_cfg = config
    else:
        selected_id, selected_score, analysis_data, selected_cfg = best

    # Mark selected
    for c in candidates:
        if c.get("candidate_id") == selected_id:
            c["decision"] = "selected"

    analysis_data.diagnostics = analysis_data.diagnostics or {}
    analysis_data.diagnostics["quality_gate"] = {
        "enabled": bool(q_enabled),
        "threshold": float(q_threshold),
        "candidates": list(candidates),
        "selected_candidate_id": str(selected_id),
        "fallbacks_triggered": list(fallbacks_triggered),
    }

    if requested_quality_mode:
        analysis_data.diagnostics["requested_quality_mode"] = requested_quality_mode
        analysis_data.diagnostics["meta_processing_mode_preserved"] = meta_processing_mode_preserved
        if meta_processing_mode_val is not None:
            analysis_data.diagnostics["meta_processing_mode_value"] = meta_processing_mode_val

    # B4: Persist beat grid
    if hasattr(analysis_data.meta, "beats") and analysis_data.meta.beats:
        analysis_data.diagnostics["beats"] = list(analysis_data.meta.beats)

    if "decision_trace" not in analysis_data.diagnostics:
        analysis_data.diagnostics["decision_trace"] = _safe_trace(base_trace)

    analysis_data.diagnostics["timing"] = {
        "total_sec": float(time.time() - t0),
        "selected_candidate_score": float(selected_score),
        "selected_candidate_id": str(selected_id),
    }

    # ---------------- Stage D ----------------
    # IMPORTANT: render using the selected candidate config (mode-specific quantize/render knobs may differ later)
    tr = quantize_and_render(analysis_data.notes, analysis_data, selected_cfg, pipeline_logger=pipeline_logger)
    return TranscriptionResult(musicxml=tr.musicxml, analysis_data=analysis_data, midi_bytes=tr.midi_bytes)
